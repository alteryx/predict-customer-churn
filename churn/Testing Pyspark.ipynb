{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1414656\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(master = 'local[*]')\n",
    "sc.defaultParallelism\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 27, 64, 125]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x ** 3, [1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 27, 64, 125]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(master = 'local[*]')\n",
    "x = sc.parallelize([1, 2, 3, 4, 5]).map(lambda x: x ** 3).collect()\n",
    "sc.stop()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(master = 'local[*]')\n",
    "sc.defaultParallelism\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 230 features.\n"
     ]
    }
   ],
   "source": [
    "import featuretools as ft\n",
    "import featuretools.variable_types as vtypes \n",
    "\n",
    "feature_defs = ft.load_features('/data/churn/features.txt')\n",
    "print(f'There are {len(feature_defs)} features.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "N_PARTITIONS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_to_feature_matrix(partition, feature_defs=feature_defs):\n",
    "    \"\"\"Take in a partition number and return a feature matrix\"\"\"\n",
    "    directory = '/data/churn/partitions/p' + str(partition)\n",
    "    \n",
    "    # Read in the data files\n",
    "    members = pd.read_csv(f'{directory}/members.csv', \n",
    "                      parse_dates=['registration_init_time'], \n",
    "                      infer_datetime_format = True, \n",
    "                      dtype = {'gender': 'category'})\n",
    "\n",
    "    trans = pd.read_csv(f'{directory}/transactions.csv',\n",
    "                       parse_dates=['transaction_date', 'membership_expire_date'], \n",
    "                        infer_datetime_format = True)\n",
    "\n",
    "    logs = pd.read_csv(f'{directory}/logs.csv', parse_dates = ['date'])\n",
    "    cutoff_times = pd.read_csv(f'{directory}/cutoff_times.csv', parse_dates = ['cutoff'])\n",
    "    cutoff_times = cutoff_times.drop_duplicates()\n",
    "    \n",
    "    # Create empty entityset\n",
    "    es = ft.EntitySet(id = 'customers')\n",
    "\n",
    "    # Add the members parent table\n",
    "    es.entity_from_dataframe(entity_id='members', dataframe=members,\n",
    "                             index = 'msno', time_index = 'registration_init_time', \n",
    "                             variable_types = {'city': vtypes.Categorical, 'bd': vtypes.Categorical,\n",
    "                                               'registered_via': vtypes.Categorical})\n",
    "    # Create new features in transactions\n",
    "    trans['price_difference'] = trans['plan_list_price'] - trans['actual_amount_paid']\n",
    "    trans['planned_daily_price'] = trans['plan_list_price'] / trans['payment_plan_days']\n",
    "    trans['daily_price'] = trans['actual_amount_paid'] / trans['payment_plan_days']\n",
    "\n",
    "    # Add the transactions child table\n",
    "    es.entity_from_dataframe(entity_id='transactions', dataframe=trans,\n",
    "                             index = 'transactions_index', make_index = True,\n",
    "                             time_index = 'transaction_date', \n",
    "                             variable_types = {'payment_method_id': vtypes.Categorical, \n",
    "                                               'is_auto_renew': vtypes.Boolean, 'is_cancel': vtypes.Boolean})\n",
    "\n",
    "    # Add transactions interesting values\n",
    "    es['transactions']['is_cancel'].interesting_values = [0, 1]\n",
    "    es['transactions']['is_auto_renew'].interesting_values = [0, 1]\n",
    "    \n",
    "    # Create new features in logs\n",
    "    logs['total'] = logs[['num_25', 'num_50', 'num_75', 'num_985', 'num_100']].sum(axis = 1)\n",
    "    logs['percent_100'] = logs['num_100'] / logs['total']\n",
    "    logs['percent_unique'] = logs['num_unq'] / logs['total']\n",
    "    \n",
    "    # Add the logs child table\n",
    "    es.entity_from_dataframe(entity_id='logs', dataframe=logs,\n",
    "                         index = 'logs_index', make_index = True,\n",
    "                         time_index = 'date')\n",
    "\n",
    "    # Add the relationships\n",
    "    r_member_transactions = ft.Relationship(es['members']['msno'], es['transactions']['msno'])\n",
    "    r_member_logs = ft.Relationship(es['members']['msno'], es['logs']['msno'])\n",
    "    es.add_relationships([r_member_transactions, r_member_logs])\n",
    "\n",
    "    # Calculate and save the feature matrix\n",
    "    feature_matrix = ft.calculate_feature_matrix(entityset=es, features=feature_defs, cutoff_time=cutoff_times)\n",
    "    \n",
    "    feature_matrix.to_csv(f'{directory}/feature_matrix.csv')\n",
    "    \n",
    "    # Report progress every 10th of number of partitions\n",
    "    if (partition % (N_PARTITIONS / 10) == 0):\n",
    "        print(f'{100 * round(partition / N_PARTITIONS)}% complete.', end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "base_dir = '/data/churn/partitions/'\n",
    "partitions = list(range(len(os.listdir(base_dir))))\n",
    "partitions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f90e9364d30>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f90e9364d30>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.dir', '/usr/local/spark/tmp'),\n",
       " ('spark.eventLog.enabled', 'True'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'pyspark-shell')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "conf.set('spark.eventLog.enabled', True);\n",
    "conf.set('spark.eventLog.dir', '/usr/local/spark/tmp');\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 27, 64, 125]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(master = 'spark://ip-172-31-23-133.ec2.internal:7077', \n",
    "                          conf = conf,\n",
    "                          appName='Cubed')\n",
    "sc.parallelize([1, 2, 3, 4, 5], numSlices=1).map(lambda x: x ** 3).collect()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App ID</th>\n",
       "      <th>App Name</th>\n",
       "      <th>Block Manager ID</th>\n",
       "      <th>Classpath Entries</th>\n",
       "      <th>Completion Time</th>\n",
       "      <th>Event</th>\n",
       "      <th>Executor ID</th>\n",
       "      <th>Executor Info</th>\n",
       "      <th>JVM Information</th>\n",
       "      <th>Job ID</th>\n",
       "      <th>Job Result</th>\n",
       "      <th>Maximum Memory</th>\n",
       "      <th>Maximum Offheap Memory</th>\n",
       "      <th>Maximum Onheap Memory</th>\n",
       "      <th>Properties</th>\n",
       "      <th>Spark Properties</th>\n",
       "      <th>Spark Version</th>\n",
       "      <th>Stage Attempt ID</th>\n",
       "      <th>Stage ID</th>\n",
       "      <th>Stage IDs</th>\n",
       "      <th>Stage Info</th>\n",
       "      <th>Stage Infos</th>\n",
       "      <th>Submission Time</th>\n",
       "      <th>System Properties</th>\n",
       "      <th>Task End Reason</th>\n",
       "      <th>Task Info</th>\n",
       "      <th>Task Metrics</th>\n",
       "      <th>Task Type</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SparkListenerLogStart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.3.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Executor ID': 'driver', 'Host': 'ip-172-31-2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SparkListenerBlockManagerAdded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>411775795.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>411775795.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-09-17 14:57:14.450</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'/usr/local/spark/jars/commons-lang-2.6.jar':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SparkListenerEnvironmentUpdate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Java Home': '/home/ubuntu/anaconda3/envs/pyt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'spark.driver.host': 'ip-172-31-23-133.ec2.in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'java.io.tmpdir': '/tmp', 'line.separator': '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>app-20180917145714-0007</td>\n",
       "      <td>Cubed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SparkListenerApplicationStart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-09-17 14:57:14.417</td>\n",
       "      <td>ubuntu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SparkListenerJobStart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'spark.rdd.scope.noOverride': 'true', 'callSi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'Stage ID': 0, 'Stage Attempt ID': 0, 'Stage...</td>\n",
       "      <td>1.537196e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    App ID App Name  \\\n",
       "0                      NaN      NaN   \n",
       "1                      NaN      NaN   \n",
       "2                      NaN      NaN   \n",
       "3  app-20180917145714-0007    Cubed   \n",
       "4                      NaN      NaN   \n",
       "\n",
       "                                    Block Manager ID  \\\n",
       "0                                                NaN   \n",
       "1  {'Executor ID': 'driver', 'Host': 'ip-172-31-2...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                   Classpath Entries  Completion Time  \\\n",
       "0                                                NaN              NaN   \n",
       "1                                                NaN              NaN   \n",
       "2  {'/usr/local/spark/jars/commons-lang-2.6.jar':...              NaN   \n",
       "3                                                NaN              NaN   \n",
       "4                                                NaN              NaN   \n",
       "\n",
       "                            Event  Executor ID Executor Info  \\\n",
       "0           SparkListenerLogStart          NaN           NaN   \n",
       "1  SparkListenerBlockManagerAdded          NaN           NaN   \n",
       "2  SparkListenerEnvironmentUpdate          NaN           NaN   \n",
       "3   SparkListenerApplicationStart          NaN           NaN   \n",
       "4           SparkListenerJobStart          NaN           NaN   \n",
       "\n",
       "                                     JVM Information  Job ID Job Result  \\\n",
       "0                                                NaN     NaN        NaN   \n",
       "1                                                NaN     NaN        NaN   \n",
       "2  {'Java Home': '/home/ubuntu/anaconda3/envs/pyt...     NaN        NaN   \n",
       "3                                                NaN     NaN        NaN   \n",
       "4                                                NaN     0.0        NaN   \n",
       "\n",
       "   Maximum Memory  Maximum Offheap Memory  Maximum Onheap Memory  \\\n",
       "0             NaN                     NaN                    NaN   \n",
       "1     411775795.0                     0.0            411775795.0   \n",
       "2             NaN                     NaN                    NaN   \n",
       "3             NaN                     NaN                    NaN   \n",
       "4             NaN                     NaN                    NaN   \n",
       "\n",
       "                                          Properties  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  {'spark.rdd.scope.noOverride': 'true', 'callSi...   \n",
       "\n",
       "                                    Spark Properties Spark Version  \\\n",
       "0                                                NaN         2.3.1   \n",
       "1                                                NaN           NaN   \n",
       "2  {'spark.driver.host': 'ip-172-31-23-133.ec2.in...           NaN   \n",
       "3                                                NaN           NaN   \n",
       "4                                                NaN           NaN   \n",
       "\n",
       "   Stage Attempt ID  Stage ID Stage IDs Stage Info  \\\n",
       "0               NaN       NaN       NaN        NaN   \n",
       "1               NaN       NaN       NaN        NaN   \n",
       "2               NaN       NaN       NaN        NaN   \n",
       "3               NaN       NaN       NaN        NaN   \n",
       "4               NaN       NaN       [0]        NaN   \n",
       "\n",
       "                                         Stage Infos  Submission Time  \\\n",
       "0                                                NaN              NaN   \n",
       "1                                                NaN              NaN   \n",
       "2                                                NaN              NaN   \n",
       "3                                                NaN              NaN   \n",
       "4  [{'Stage ID': 0, 'Stage Attempt ID': 0, 'Stage...     1.537196e+12   \n",
       "\n",
       "                                   System Properties Task End Reason  \\\n",
       "0                                                NaN             NaN   \n",
       "1                                                NaN             NaN   \n",
       "2  {'java.io.tmpdir': '/tmp', 'line.separator': '...             NaN   \n",
       "3                                                NaN             NaN   \n",
       "4                                                NaN             NaN   \n",
       "\n",
       "  Task Info Task Metrics Task Type               Timestamp    User  \n",
       "0       NaN          NaN       NaN                     NaT     NaN  \n",
       "1       NaN          NaN       NaN 2018-09-17 14:57:14.450     NaN  \n",
       "2       NaN          NaN       NaN                     NaT     NaN  \n",
       "3       NaN          NaN       NaN 2018-09-17 14:57:14.417  ubuntu  \n",
       "4       NaN          NaN       NaN                     NaT     NaN  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('/usr/local/spark/tmp/app-20180917145714-0007', lines = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 0.0 failed 1 times, most recent failure: Lost task 9.0 in stage 0.0 (TID 9, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-143-4bd6d5ccd8af>\", line 59, in partition_to_feature_matrix\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 258, in calculate_feature_matrix\n    pass_columns=pass_columns)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 520, in linear_calculate_chunks\n    backend=backend)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 342, in calculate_chunk\n    training_window=window)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/utils.py\", line 34, in wrapped\n    r = method(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 316, in calc_results\n    profile=profile)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/pandas_backend.py\", line 196, in calculate_all_features\n    result_frame = handler(group, input_frames)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/pandas_backend.py\", line 442, in _calculate_agg_features\n    to_merge.reset_index(1, drop=True, inplace=True)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\", line 4101, in reset_index\n    level = [self.index._get_level_number(lev) for lev in level]\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\", line 4101, in <listcomp>\n    level = [self.index._get_level_number(lev) for lev in level]\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 1961, in _get_level_number\n    self._validate_index_level(level)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 1955, in _validate_index_level\n    (level + 1))\nIndexError: Too many levels: Index has only 1 level, not 2\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-143-4bd6d5ccd8af>\", line 59, in partition_to_feature_matrix\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 258, in calculate_feature_matrix\n    pass_columns=pass_columns)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 520, in linear_calculate_chunks\n    backend=backend)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 342, in calculate_chunk\n    training_window=window)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/utils.py\", line 34, in wrapped\n    r = method(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 316, in calc_results\n    profile=profile)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/pandas_backend.py\", line 196, in calculate_all_features\n    result_frame = handler(group, input_frames)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/pandas_backend.py\", line 442, in _calculate_agg_features\n    to_merge.reset_index(1, drop=True, inplace=True)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\", line 4101, in reset_index\n    level = [self.index._get_level_number(lev) for lev in level]\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\", line 4101, in <listcomp>\n    level = [self.index._get_level_number(lev) for lev in level]\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 1961, in _get_level_number\n    self._validate_index_level(level)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 1955, in _validate_index_level\n    (level + 1))\nIndexError: Too many levels: Index has only 1 level, not 2\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-cffa056fbd2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m sc = pyspark.SparkContext(master = 'local[*]', \n\u001b[1;32m      3\u001b[0m                           appName = 'featuretools', conf = conf)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition_to_feature_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \"\"\"\n\u001b[1;32m    833\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 0.0 failed 1 times, most recent failure: Lost task 9.0 in stage 0.0 (TID 9, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-143-4bd6d5ccd8af>\", line 59, in partition_to_feature_matrix\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 258, in calculate_feature_matrix\n    pass_columns=pass_columns)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 520, in linear_calculate_chunks\n    backend=backend)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 342, in calculate_chunk\n    training_window=window)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/utils.py\", line 34, in wrapped\n    r = method(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 316, in calc_results\n    profile=profile)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/pandas_backend.py\", line 196, in calculate_all_features\n    result_frame = handler(group, input_frames)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/pandas_backend.py\", line 442, in _calculate_agg_features\n    to_merge.reset_index(1, drop=True, inplace=True)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\", line 4101, in reset_index\n    level = [self.index._get_level_number(lev) for lev in level]\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\", line 4101, in <listcomp>\n    level = [self.index._get_level_number(lev) for lev in level]\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 1961, in _get_level_number\n    self._validate_index_level(level)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 1955, in _validate_index_level\n    (level + 1))\nIndexError: Too many levels: Index has only 1 level, not 2\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-143-4bd6d5ccd8af>\", line 59, in partition_to_feature_matrix\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 258, in calculate_feature_matrix\n    pass_columns=pass_columns)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 520, in linear_calculate_chunks\n    backend=backend)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 342, in calculate_chunk\n    training_window=window)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/utils.py\", line 34, in wrapped\n    r = method(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/calculate_feature_matrix.py\", line 316, in calc_results\n    profile=profile)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/pandas_backend.py\", line 196, in calculate_all_features\n    result_frame = handler(group, input_frames)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/featuretools/computational_backends/pandas_backend.py\", line 442, in _calculate_agg_features\n    to_merge.reset_index(1, drop=True, inplace=True)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\", line 4101, in reset_index\n    level = [self.index._get_level_number(lev) for lev in level]\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\", line 4101, in <listcomp>\n    level = [self.index._get_level_number(lev) for lev in level]\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 1961, in _get_level_number\n    self._validate_index_level(level)\n  File \"/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 1955, in _validate_index_level\n    (level + 1))\nIndexError: Too many levels: Index has only 1 level, not 2\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "sc = pyspark.SparkContext(master = 'local[*]', \n",
    "                          appName = 'featuretools', conf = conf)\n",
    "r = sc.parallelize(partitions, numSlices=1000).map(partition_to_feature_matrix).collect()\n",
    "sc.stop()\n",
    "end = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App ID</th>\n",
       "      <th>App Name</th>\n",
       "      <th>Block Manager ID</th>\n",
       "      <th>Classpath Entries</th>\n",
       "      <th>Completion Time</th>\n",
       "      <th>Event</th>\n",
       "      <th>Executor ID</th>\n",
       "      <th>Executor Info</th>\n",
       "      <th>JVM Information</th>\n",
       "      <th>Job ID</th>\n",
       "      <th>...</th>\n",
       "      <th>Stage Info</th>\n",
       "      <th>Stage Infos</th>\n",
       "      <th>Submission Time</th>\n",
       "      <th>System Properties</th>\n",
       "      <th>Task End Reason</th>\n",
       "      <th>Task Info</th>\n",
       "      <th>Task Metrics</th>\n",
       "      <th>Task Type</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SparkListenerLogStart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Executor ID': 'driver', 'Host': 'ip-172-31-2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SparkListenerBlockManagerAdded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-09-17 14:58:54.536</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'/usr/local/spark/jars/commons-lang-2.6.jar':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SparkListenerEnvironmentUpdate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Java Home': '/home/ubuntu/anaconda3/envs/pyt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'java.io.tmpdir': '/tmp', 'line.separator': '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>app-20180917145854-0009</td>\n",
       "      <td>testing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SparkListenerApplicationStart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-09-17 14:58:54.487</td>\n",
       "      <td>ubuntu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SparkListenerJobStart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'Stage ID': 0, 'Stage Attempt ID': 0, 'Stage...</td>\n",
       "      <td>1.537196e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    App ID App Name  \\\n",
       "0                      NaN      NaN   \n",
       "1                      NaN      NaN   \n",
       "2                      NaN      NaN   \n",
       "3  app-20180917145854-0009  testing   \n",
       "4                      NaN      NaN   \n",
       "\n",
       "                                    Block Manager ID  \\\n",
       "0                                                NaN   \n",
       "1  {'Executor ID': 'driver', 'Host': 'ip-172-31-2...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                   Classpath Entries  Completion Time  \\\n",
       "0                                                NaN              NaN   \n",
       "1                                                NaN              NaN   \n",
       "2  {'/usr/local/spark/jars/commons-lang-2.6.jar':...              NaN   \n",
       "3                                                NaN              NaN   \n",
       "4                                                NaN              NaN   \n",
       "\n",
       "                            Event  Executor ID Executor Info  \\\n",
       "0           SparkListenerLogStart          NaN           NaN   \n",
       "1  SparkListenerBlockManagerAdded          NaN           NaN   \n",
       "2  SparkListenerEnvironmentUpdate          NaN           NaN   \n",
       "3   SparkListenerApplicationStart          NaN           NaN   \n",
       "4           SparkListenerJobStart          NaN           NaN   \n",
       "\n",
       "                                     JVM Information  Job ID   ...    \\\n",
       "0                                                NaN     NaN   ...     \n",
       "1                                                NaN     NaN   ...     \n",
       "2  {'Java Home': '/home/ubuntu/anaconda3/envs/pyt...     NaN   ...     \n",
       "3                                                NaN     NaN   ...     \n",
       "4                                                NaN     0.0   ...     \n",
       "\n",
       "  Stage Info                                        Stage Infos  \\\n",
       "0        NaN                                                NaN   \n",
       "1        NaN                                                NaN   \n",
       "2        NaN                                                NaN   \n",
       "3        NaN                                                NaN   \n",
       "4        NaN  [{'Stage ID': 0, 'Stage Attempt ID': 0, 'Stage...   \n",
       "\n",
       "   Submission Time                                  System Properties  \\\n",
       "0              NaN                                                NaN   \n",
       "1              NaN                                                NaN   \n",
       "2              NaN  {'java.io.tmpdir': '/tmp', 'line.separator': '...   \n",
       "3              NaN                                                NaN   \n",
       "4     1.537196e+12                                                NaN   \n",
       "\n",
       "  Task End Reason Task Info Task Metrics  Task Type               Timestamp  \\\n",
       "0             NaN       NaN          NaN        NaN                     NaT   \n",
       "1             NaN       NaN          NaN        NaN 2018-09-17 14:58:54.536   \n",
       "2             NaN       NaN          NaN        NaN                     NaT   \n",
       "3             NaN       NaN          NaN        NaN 2018-09-17 14:58:54.487   \n",
       "4             NaN       NaN          NaN        NaN                     NaT   \n",
       "\n",
       "     User  \n",
       "0     NaN  \n",
       "1     NaN  \n",
       "2     NaN  \n",
       "3  ubuntu  \n",
       "4     NaN  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('/usr/local/spark/tmp/app-20180917145854-0009', lines = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block finds the time in seconds to complete each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         NaN\n",
       "1         NaN\n",
       "2         NaN\n",
       "3         NaN\n",
       "4         NaN\n",
       "5         NaN\n",
       "6         NaN\n",
       "7         0.0\n",
       "8         1.0\n",
       "9         2.0\n",
       "10        3.0\n",
       "11        4.0\n",
       "12        5.0\n",
       "13        6.0\n",
       "14        7.0\n",
       "15        8.0\n",
       "16        9.0\n",
       "17       10.0\n",
       "18       11.0\n",
       "19       12.0\n",
       "20       13.0\n",
       "21       14.0\n",
       "22       15.0\n",
       "23        NaN\n",
       "24       16.0\n",
       "25       12.0\n",
       "26       17.0\n",
       "27        4.0\n",
       "28       18.0\n",
       "29       10.0\n",
       "        ...  \n",
       "1981    978.0\n",
       "1982    995.0\n",
       "1983    981.0\n",
       "1984    996.0\n",
       "1985    980.0\n",
       "1986    997.0\n",
       "1987    979.0\n",
       "1988    998.0\n",
       "1989    983.0\n",
       "1990    999.0\n",
       "1991    985.0\n",
       "1992    984.0\n",
       "1993    982.0\n",
       "1994    986.0\n",
       "1995    987.0\n",
       "1996    988.0\n",
       "1997    989.0\n",
       "1998    992.0\n",
       "1999    990.0\n",
       "2000    991.0\n",
       "2001    993.0\n",
       "2002    994.0\n",
       "2003    995.0\n",
       "2004    999.0\n",
       "2005    997.0\n",
       "2006    996.0\n",
       "2007    998.0\n",
       "2008      NaN\n",
       "2009      NaN\n",
       "2010      NaN\n",
       "Name: Task ID, Length: 2011, dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_task_info(task_dict):\n",
    "    try:\n",
    "        return task_dict.get('Task ID')\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "df['Task ID'] = df['Task Info'].apply(filter_task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App ID</th>\n",
       "      <th>App Name</th>\n",
       "      <th>Block Manager ID</th>\n",
       "      <th>Classpath Entries</th>\n",
       "      <th>Completion Time</th>\n",
       "      <th>Event</th>\n",
       "      <th>Executor ID</th>\n",
       "      <th>Executor Info</th>\n",
       "      <th>JVM Information</th>\n",
       "      <th>Job ID</th>\n",
       "      <th>Job Result</th>\n",
       "      <th>Maximum Memory</th>\n",
       "      <th>Maximum Offheap Memory</th>\n",
       "      <th>Maximum Onheap Memory</th>\n",
       "      <th>Properties</th>\n",
       "      <th>Spark Properties</th>\n",
       "      <th>Spark Version</th>\n",
       "      <th>Stage Attempt ID</th>\n",
       "      <th>Stage ID</th>\n",
       "      <th>Stage IDs</th>\n",
       "      <th>Stage Info</th>\n",
       "      <th>Stage Infos</th>\n",
       "      <th>Submission Time</th>\n",
       "      <th>System Properties</th>\n",
       "      <th>Task End Reason</th>\n",
       "      <th>Task Info</th>\n",
       "      <th>Task Metrics</th>\n",
       "      <th>Task Type</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User</th>\n",
       "      <th>Task ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SparkListenerTaskStart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Task ID': 996, 'Index': 996, 'Attempt': 0, '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>996.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SparkListenerTaskEnd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Reason': 'Success'}</td>\n",
       "      <td>{'Task ID': 996, 'Index': 996, 'Attempt': 0, '...</td>\n",
       "      <td>{'Executor Deserialize Time': 0, 'Executor Des...</td>\n",
       "      <td>ResultTask</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>996.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     App ID App Name Block Manager ID Classpath Entries  Completion Time  \\\n",
       "1984    NaN      NaN              NaN               NaN              NaN   \n",
       "2006    NaN      NaN              NaN               NaN              NaN   \n",
       "\n",
       "                       Event  Executor ID Executor Info JVM Information  \\\n",
       "1984  SparkListenerTaskStart          NaN           NaN             NaN   \n",
       "2006    SparkListenerTaskEnd          NaN           NaN             NaN   \n",
       "\n",
       "      Job ID Job Result  Maximum Memory  Maximum Offheap Memory  \\\n",
       "1984     NaN        NaN             NaN                     NaN   \n",
       "2006     NaN        NaN             NaN                     NaN   \n",
       "\n",
       "      Maximum Onheap Memory Properties Spark Properties Spark Version  \\\n",
       "1984                    NaN        NaN              NaN           NaN   \n",
       "2006                    NaN        NaN              NaN           NaN   \n",
       "\n",
       "      Stage Attempt ID  Stage ID Stage IDs Stage Info Stage Infos  \\\n",
       "1984               0.0       0.0       NaN        NaN         NaN   \n",
       "2006               0.0       0.0       NaN        NaN         NaN   \n",
       "\n",
       "      Submission Time System Properties        Task End Reason  \\\n",
       "1984              NaN               NaN                    NaN   \n",
       "2006              NaN               NaN  {'Reason': 'Success'}   \n",
       "\n",
       "                                              Task Info  \\\n",
       "1984  {'Task ID': 996, 'Index': 996, 'Attempt': 0, '...   \n",
       "2006  {'Task ID': 996, 'Index': 996, 'Attempt': 0, '...   \n",
       "\n",
       "                                           Task Metrics   Task Type Timestamp  \\\n",
       "1984                                                NaN         NaN       NaT   \n",
       "2006  {'Executor Deserialize Time': 0, 'Executor Des...  ResultTask       NaT   \n",
       "\n",
       "     User  Task ID  \n",
       "1984  NaN    996.0  \n",
       "2006  NaN    996.0  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 40\n",
    "df[df['Task ID'] == 996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Task ID': 996,\n",
       " 'Index': 996,\n",
       " 'Attempt': 0,\n",
       " 'Launch Time': 1537196517065,\n",
       " 'Executor ID': '0',\n",
       " 'Host': '172.31.23.133',\n",
       " 'Locality': 'PROCESS_LOCAL',\n",
       " 'Speculative': False,\n",
       " 'Getting Result Time': 0,\n",
       " 'Finish Time': 0,\n",
       " 'Failed': False,\n",
       " 'Killed': False,\n",
       " 'Accumulables': []}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1984, 'Task Info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2809"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[2006, 'Task Info']['Finish Time'] - df.loc[2006, 'Task Info']['Launch Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-09-17 15:01:57.065000')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(1537196517065, unit = 'ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
